# Energy-Aware Adaptive RAG

A research project exploring **compute-driven discovery of optimal RAG strategies** through reinforcement learning. Instead of hand-crafted routing rules, we train a small controller to dynamically decide *when to retrieve*, *which model to use*, and *when to stop* â€” optimizing for both accuracy and energy efficiency.

## Research Vision

### The Problem
RAG systems face a fundamental cost-quality tradeoff:
- **Retrieve too little** â†’ hallucination
- **Retrieve too much** â†’ slow, expensive, and noisy context
- **Use a big model** â†’ accurate but costly
- **Use a small model** â†’ fast but error-prone

Current solutions rely on **human-designed heuristics** (query complexity rules, confidence thresholds). We take a different approach inspired by the "Bitter Lesson": **let compute discover the optimal policy**.

### The Solution: Green-DeepRAG
An **iterative Manager-Worker agent** where a small encoder (the "Manager") routes tasks to frozen LLM workers, receiving only **compressed observations** back â€” never raw documents.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         MANAGER-WORKER LOOP                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   [CLS] Query [SEP] Step1_Summary [SEP] Step2_Summary [SEP]                 â”‚
â”‚                           â”‚                                                 â”‚
â”‚                           â–¼                                                 â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚                  â”‚   Controller    â”‚  (RoBERTa-Large)                       â”‚
â”‚                  â”‚   "The Manager" â”‚                                        â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚                           â”‚ classifies â†’ Action ID (0-6)                    â”‚
â”‚                           â–¼                                                 â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚         â”‚            WORKER EXECUTION             â”‚                         â”‚
â”‚         â”‚  (SLM/LLM executes, returns <50 token   â”‚                         â”‚
â”‚         â”‚   summary â€” Manager never sees raw docs) â”‚                         â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                           â”‚                                                 â”‚
â”‚                           â–¼                                                 â”‚
â”‚              "Found 3 docs on Apple revenue.                                â”‚
â”‚               Missing 2024 data."  (Observation)                            â”‚
â”‚                           â”‚                                                 â”‚
â”‚                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Append to state, loop back           â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Insight**: The controller never breaches its 512-token context limit because workers compress all intermediate results into short status updates.

**Why Encoder over Decoder?** We don't need to *generate* text â€” we only need to *route*. BERT-style encoders provide better bidirectional state understanding per parameter and faster inference than autoregressive decoders.

## Architecture

### Components

| Component | Role | Example |
|-----------|------|---------|
| **Controller** | Encoder that classifies state â†’ action (Manager) | RoBERTa-Large, DeBERTa-v3 |
| **SLM Worker** | Fast, cheap generation + compression | Mistral-7B, Llama-3-8B |
| **LLM Worker** | Expensive, accurate generation + compression | Llama-3-70B, GPT-4o |
| **Retriever** | BM25 or dense search | rank_bm25, Faiss |
| **Judge** | Validates answer correctness | Exact match + LLM-judge |

### Action Space (7 Classes)
The controller outputs a probability distribution over 7 discrete actions:

| ID | Action | Description |
|----|--------|-------------|
| 0 | `Generate_and_End(SLM)` | Final answer with small model |
| 1 | `Generate_and_End(LLM)` | Final answer with large model |
| 2 | `Decompose(SLM)` | Break query into sub-questions |
| 3 | `Decompose(LLM)` | Break query into sub-questions |
| 4 | `Retrieve(Keyword)` | BM25 search |
| 5 | `Retrieve(Dense)` | Vector similarity search |
| 6 | `Reason(LLM)` | Intermediate synthesis/verification |

**Costs are measured via CodeCarbon** on target hardware, not hard-coded. A cost table is pre-computed by benchmarking each action.

**Input format:** `[CLS] Original_Query [SEP] Step_1_Summary [SEP] Step_2_Summary [SEP] ...`  
**Output:** Softmax over 7 action logits

### The Critical Constraint: State Compression

Workers **never pass raw documents** to the controller. Instead:
1. Worker executes action (e.g., retrieves 2000 tokens)
2. Worker generates a **<50 token status update**
3. Status update is appended to controller's state

This ensures the encoder never exceeds 512 tokens while retaining semantic signal.

### Training Pipeline (3 Phases)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Phase 1: Cost-Ordered Search (Offline Oracle)                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  For each query, simulate agent with GreenTreeSearch:                       â”‚
â”‚    - Try actions in ascending cost order                                    â”‚
â”‚    - Generate compressed observations at each step                          â”‚
â”‚    - Record cheapest trajectory that yields correct answer                  â”‚
â”‚  Output: Dataset of (state â†’ action) pairs with compressed observations     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Phase 2: Behavior Cloning (Classification)                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Train RoBERTa classifier with Cross-Entropy loss on Phase 1 traces         â”‚
â”‚  Input: [CLS] query [SEP] obs_1 [SEP] obs_2 ...  â†’  Output: action (0-6)    â”‚
â”‚  Output: Policy that mimics "cheapest winner" trajectories                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Phase 3: Cost-Aware PPO (Refinement)                                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Online RL with reward: R = Î±Â·I(Correct) - Î²Â·Î£ Energy(actions)              â”‚
â”‚  Agent learns to balance cheap failures vs expensive successes              â”‚
â”‚  Output: Energy-aware adaptive RAG controller                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Project Status

| Phase | Status | Description |
|-------|--------|-------------|
| **Phase 0** | âœ… Complete | Baselines & infrastructure (BM25 retrieval, evaluation harness, energy tracking) |
| **Phase 1** | ğŸ”„ In Progress | Cost-ordered search with compressed observations |
| **Phase 2** | âŒ Pending | Behavior cloning on generated traces |
| **Phase 3** | âŒ Pending | PPO refinement with energy-aware reward |

### Current Baseline Results (HotpotQA, 100 questions)

| Metric | Dense Retrieval | BM25 Retrieval | Î” |
|--------|-----------------|----------------|---|
| Claim Recall | 39.6% | 51.2% | +11.6% |
| Hallucination | 42.5% | 22.8% | **-19.7%** |
| Faithfulness | 53.5% | 73.2% | **+19.7%** |
| Context Utilization | 30.5% | 40.7% | +10.2% |

## Project Structure

```
rag_eval/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ corpus.py         # Corpus loading and chunking
â”‚   â”œâ”€â”€ retriever.py      # BM25 and Faiss retrievers
â”‚   â”œâ”€â”€ generator.py      # LLM generation (Ollama/Gemini)
â”‚   â”œâ”€â”€ pipeline.py       # RAG orchestration
â”‚   â””â”€â”€ base.py           # BaseRAG interface
â”œâ”€â”€ baselines/
â”‚   â”œâ”€â”€ naive_rag.py      # Standard k=5 retrieval
â”‚   â”œâ”€â”€ fullk_rag.py      # Exhaustive k=50 retrieval
â”‚   â”œâ”€â”€ no_retrieval.py   # Generator-only baseline
â”‚   â””â”€â”€ adaptive_rag.py   # Rule-based routing (comparison)
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ harness.py        # Minimal eval (EM, F1)
â”‚   â””â”€â”€ energy.py         # CodeCarbon energy tracking
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_comparison.py # Compare all baselines
â”‚   â””â”€â”€ build_hotpotqa_distractor_corpus.py
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ run_baseline.py   # Main evaluation script
â”‚   â””â”€â”€ logs/             # Evaluation results
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/        # Chunked passages (66k from HotpotQA)
â”‚   â””â”€â”€ indexes/          # BM25 and Faiss indexes
â”œâ”€â”€ config_local.yaml     # Configuration (Ollama)
â””â”€â”€ PROJECT_PLAN.md       # Detailed research plan
```

## Setup

### 1. Install Dependencies

```bash
pip install -r requirements.txt
python -m nltk.downloader punkt
```

### 2. Set Up Ollama (Local LLMs)

```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Start server
ollama serve

# Pull models
ollama pull mistral          # SLM worker (7B)
ollama pull llama3:70b       # LLM worker (optional, requires >40GB VRAM)
```

See [OLLAMA_SETUP.md](OLLAMA_SETUP.md) for detailed instructions.

### 3. Configure

Edit `config_local.yaml`:

```yaml
retriever:
  type: "bm25"              # or "dense" for vector search
  index_path: "./data/indexes/faiss.bm25.pkl"

generator:
  type: "ollama"
  model: "mistral"          # SLM worker
  temperature: 0.0
```

## Usage

### Run Baseline Comparison

```bash
python scripts/run_comparison.py --config config_local.yaml --num-questions 50
```

Compares: naive_k5, full_k50, no_retrieval, adaptive_rule

### Run Full Evaluation (with RAGChecker)

```bash
python experiments/run_baseline.py --config config_local.yaml
```

### Build Corpus & Index

```bash
# Build HotpotQA corpus with distractor paragraphs
python scripts/build_hotpotqa_distractor_corpus.py

# Build BM25 index
python src/retriever.py
```

## Baseline Strategies

| Strategy | Description | Use Case |
|----------|-------------|----------|
| `naive_k5` | Retrieve top-5, generate once | Standard RAG baseline |
| `full_k50` | Retrieve top-50, generate once | Maximum recall baseline |
| `no_retrieval` | Generate from LLM knowledge only | Lower bound (parametric only) |
| `adaptive_rule` | Route based on query complexity | Heuristic comparison |

## Key Files for Phase 1

To implement the cost-ordered search, you'll need:

```python
# green_tree_search.py (to be implemented)
class GreenTreeSearch:
    def __init__(self, slm, llm, retriever, judge):
        self.costs = {"slm": 1, "retrieve": 5, "llm": 20}
    
    def search(self, query, ground_truth):
        # Try paths in ascending cost order
        # Return cheapest successful trajectory
```

## Dependencies

- **Retrieval**: `rank_bm25`, `faiss-cpu`, `sentence-transformers`
- **Generation**: `ollama`, `google-generativeai`
- **Evaluation**: `ragchecker`, `datasets`
- **Energy**: `codecarbon`
- **RL (Phase 3)**: `trl`, `stable-baselines3`

## References

- [Adaptive-RAG](https://arxiv.org/abs/2403.14403) â€” Query complexity routing
- [DeepRAG](https://arxiv.org/abs/2502.01142) â€” Multi-hop retrieval with atomic actions
- [The Bitter Lesson](http://www.incompleteideas.net/IncsIdeas/BitterLesson.html) â€” Compute > human knowledge
- [RAGChecker](https://arxiv.org/abs/2408.08067) â€” Fine-grained RAG evaluation

## Citation

```bibtex
@misc{energyawarerag2025,
  title={Energy-Aware Adaptive RAG via Reinforcement Learning},
  author={...},
  year={2025}
}
```