# Energy-Aware Adaptive RAG

A research project exploring **compute-driven discovery of optimal RAG strategies** through reinforcement learning. Instead of hand-crafted routing rules, we train a small controller to dynamically decide *when to retrieve*, *which model to use*, and *when to stop* â€” optimizing for both accuracy and energy efficiency.

## Research Vision

### The Problem
RAG systems face a fundamental cost-quality tradeoff:
- **Retrieve too little** â†’ hallucination
- **Retrieve too much** â†’ slow, expensive, and noisy context
- **Use a big model** â†’ accurate but costly
- **Use a small model** â†’ fast but error-prone

Current solutions rely on **human-designed heuristics** (query complexity rules, confidence thresholds). We take a different approach inspired by the "Bitter Lesson": **let compute discover the optimal policy**.

### The Solution: Green-DeepRAG
A sequential decision agent that learns to route queries through the cheapest successful path:

```
Query â†’ [Controller] â†’ <RETRIEVE>? â†’ <ASSIGN_SLM>/<ASSIGN_LLM>? â†’ <STOP>
              â†“
        Tiny Decoder (1B params)
        Trained via RL to minimize: Energy + Maximize: Accuracy
```

**Key Insight**: Most queries don't need expensive retrieval + large LLM. A small model can handle simple factual questions; retrieval is only needed for knowledge-intensive queries; large models are reserved for complex reasoning.

## Architecture

### Components

| Component | Role | Example |
|-----------|------|---------|
| **Controller** | Tiny decoder that emits control tokens | Qwen-2.5-0.5B, SmolLM-1.7B |
| **SLM Worker** | Fast, cheap generation | Mistral-7B, Llama-3-8B |
| **LLM Worker** | Expensive, accurate generation | Llama-3-70B, GPT-4o |
| **Retriever** | BM25 keyword search | rank_bm25 |
| **Judge** | Validates answer correctness | Exact match + LLM-judge |

### Action Space
The controller generates control tokens to orchestrate the pipeline:
- `<RETRIEVE>` â€” Call the retriever
- `<ASSIGN_SLM>` â€” Generate with small model
- `<ASSIGN_LLM>` â€” Generate with large model  
- `<STOP>` â€” Emit final answer

### Training Pipeline (3 Phases)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Phase 1: Cost-Ordered Search (Offline Oracle)                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  For each query, try paths in ascending cost order:                         â”‚
â”‚    1. SLM Direct (cost=1) â†’ if correct, save trace                          â”‚
â”‚    2. Retrieve + SLM (cost=6) â†’ if correct, save trace                      â”‚
â”‚    3. LLM Direct (cost=20) â†’ if correct, save trace                         â”‚
â”‚    4. Retrieve + LLM (cost=25) â†’ fallback                                   â”‚
â”‚  Output: Dataset of (query â†’ cheapest successful trajectory)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Phase 2: Behavior Cloning (Warm Start)                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Supervised fine-tuning of Controller on Phase 1 traces                     â”‚
â”‚  Output: Policy that mimics "cheapest winner" heuristic                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Phase 3: Cost-Aware PPO (Refinement)                                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Online RL with reward: R = Î±Â·I(Correct) - Î²Â·Energy(trajectory)             â”‚
â”‚  Agent can deviate from greedy path to find better tradeoffs                â”‚
â”‚  Output: Energy-aware adaptive RAG controller                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Project Status

| Phase | Status | Description |
|-------|--------|-------------|
| **Phase 0** | âœ… Complete | Baselines & infrastructure (BM25 retrieval, evaluation harness, energy tracking) |
| **Phase 1** | ðŸ”„ In Progress | Cost-ordered search data synthesis |
| **Phase 2** | âŒ Pending | Behavior cloning on generated traces |
| **Phase 3** | âŒ Pending | PPO refinement with energy-aware reward |

### Current Baseline Results (HotpotQA, 100 questions)

| Metric | Dense Retrieval | BM25 Retrieval | Î” |
|--------|-----------------|----------------|---|
| Claim Recall | 39.6% | 51.2% | +11.6% |
| Hallucination | 42.5% | 22.8% | **-19.7%** |
| Faithfulness | 53.5% | 73.2% | **+19.7%** |
| Context Utilization | 30.5% | 40.7% | +10.2% |

## Project Structure

```
rag_eval/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ corpus.py         # Corpus loading and chunking
â”‚   â”œâ”€â”€ retriever.py      # BM25 and Faiss retrievers
â”‚   â”œâ”€â”€ generator.py      # LLM generation (Ollama/Gemini)
â”‚   â”œâ”€â”€ pipeline.py       # RAG orchestration
â”‚   â””â”€â”€ base.py           # BaseRAG interface
â”œâ”€â”€ baselines/
â”‚   â”œâ”€â”€ naive_rag.py      # Standard k=5 retrieval
â”‚   â”œâ”€â”€ fullk_rag.py      # Exhaustive k=50 retrieval
â”‚   â”œâ”€â”€ no_retrieval.py   # Generator-only baseline
â”‚   â””â”€â”€ adaptive_rag.py   # Rule-based routing (comparison)
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ harness.py        # Minimal eval (EM, F1)
â”‚   â””â”€â”€ energy.py         # CodeCarbon energy tracking
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_comparison.py # Compare all baselines
â”‚   â””â”€â”€ build_hotpotqa_distractor_corpus.py
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ run_baseline.py   # Main evaluation script
â”‚   â””â”€â”€ logs/             # Evaluation results
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/        # Chunked passages (66k from HotpotQA)
â”‚   â””â”€â”€ indexes/          # BM25 and Faiss indexes
â”œâ”€â”€ config_local.yaml     # Configuration (Ollama)
â””â”€â”€ PROJECT_PLAN.md       # Detailed research plan
```

## Setup

### 1. Install Dependencies

```bash
pip install -r requirements.txt
python -m nltk.downloader punkt
```

### 2. Set Up Ollama (Local LLMs)

```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Start server
ollama serve

# Pull models
ollama pull mistral          # SLM worker (7B)
ollama pull llama3:70b       # LLM worker (optional, requires >40GB VRAM)
```

See [OLLAMA_SETUP.md](OLLAMA_SETUP.md) for detailed instructions.

### 3. Configure

Edit `config_local.yaml`:

```yaml
retriever:
  type: "bm25"              # or "dense" for vector search
  index_path: "./data/indexes/faiss.bm25.pkl"

generator:
  type: "ollama"
  model: "mistral"          # SLM worker
  temperature: 0.0
```

## Usage

### Run Baseline Comparison

```bash
python scripts/run_comparison.py --config config_local.yaml --num-questions 50
```

Compares: naive_k5, full_k50, no_retrieval, adaptive_rule

### Run Full Evaluation (with RAGChecker)

```bash
python experiments/run_baseline.py --config config_local.yaml
```

### Build Corpus & Index

```bash
# Build HotpotQA corpus with distractor paragraphs
python scripts/build_hotpotqa_distractor_corpus.py

# Build BM25 index
python src/retriever.py
```

## Baseline Strategies

| Strategy | Description | Use Case |
|----------|-------------|----------|
| `naive_k5` | Retrieve top-5, generate once | Standard RAG baseline |
| `full_k50` | Retrieve top-50, generate once | Maximum recall baseline |
| `no_retrieval` | Generate from LLM knowledge only | Lower bound (parametric only) |
| `adaptive_rule` | Route based on query complexity | Heuristic comparison |

## Key Files for Phase 1

To implement the cost-ordered search, you'll need:

```python
# green_tree_search.py (to be implemented)
class GreenTreeSearch:
    def __init__(self, slm, llm, retriever, judge):
        self.costs = {"slm": 1, "retrieve": 5, "llm": 20}
    
    def search(self, query, ground_truth):
        # Try paths in ascending cost order
        # Return cheapest successful trajectory
```

## Dependencies

- **Retrieval**: `rank_bm25`, `faiss-cpu`, `sentence-transformers`
- **Generation**: `ollama`, `google-generativeai`
- **Evaluation**: `ragchecker`, `datasets`
- **Energy**: `codecarbon`
- **RL (Phase 3)**: `trl`, `stable-baselines3`

## References

- [Adaptive-RAG](https://arxiv.org/abs/2403.14403) â€” Query complexity routing
- [DeepRAG](https://arxiv.org/abs/2502.01142) â€” Multi-hop retrieval with atomic actions
- [The Bitter Lesson](http://www.incompleteideas.net/IncsIdeas/BitterLesson.html) â€” Compute > human knowledge
- [RAGChecker](https://arxiv.org/abs/2408.08067) â€” Fine-grained RAG evaluation

## Citation

```bibtex
@misc{energyawarerag2025,
  title={Energy-Aware Adaptive RAG via Reinforcement Learning},
  author={...},
  year={2025}
}
```